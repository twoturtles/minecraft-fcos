{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1604dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3911f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision.transforms import v2 as v2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import bb\n",
    "import tt\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "tt.logging_init()\n",
    "\n",
    "SEED = 325\n",
    "tt.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613356d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.home() / \"src/data\"\n",
    "mc_data_path = data_path / \"minecraft/info.json\"\n",
    "dset = bb.Dataset.load(mc_data_path)\n",
    "torch_root = data_path / \"torchvision\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b4896",
   "metadata": {},
   "source": [
    "# bb.TorchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3713825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdset = bb.TorchDataset(data_path / \"minecraft\")\n",
    "tdset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(tdset, batch_size=8, collate_fn=bb.TorchDataset.collate_fn)\n",
    "images, targets = next(iter(loader))\n",
    "result = tv.utils.make_grid(\n",
    "    [bb.torch_plot_bb(img, target, tdset.categories) for img, target in zip(images, targets)], nrow=2\n",
    ")\n",
    "v2.functional.to_pil_image(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = tdset[10]\n",
    "categories = tdset.dset.categories\n",
    "label_names = [categories[label.item()] for label in target[\"labels\"]]\n",
    "result = bb.torch_plot_bb(img, target, tdset.categories)\n",
    "v2.functional.to_pil_image(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0e2f9",
   "metadata": {},
   "source": [
    "# Minecraft COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a42052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.pytorch.org/vision/main/auto_examples/transforms/plot_transforms_e2e.html\n",
    "\n",
    "IMAGES_PATH = data_path / \"coco/minecraft/images\"\n",
    "ANNOTATIONS_PATH = data_path / \"coco/minecraft/annotations.json\"\n",
    "coco_dataset = tv.datasets.wrap_dataset_for_transforms_v2(\n",
    "    # The transforms can be v2 since they're handled by the wrapper.\n",
    "    tv.datasets.CocoDetection(IMAGES_PATH, ANNOTATIONS_PATH, transforms=v2.ToImage())\n",
    ")\n",
    "\n",
    "coco_categories = {\n",
    "    cat[\"id\"]: cat[\"name\"] for cat in coco_dataset.coco.loadCats(coco_dataset.coco.getCatIds())\n",
    "}\n",
    "print(coco_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = coco_dataset[0]\n",
    "label_names = [coco_categories[label.item()] for label in target[\"labels\"]]\n",
    "print(target)\n",
    "print(label_names)\n",
    "v2.ToPILImage()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(\n",
    "    batch: list[tuple[tv.tv_tensors.Image, dict[str, Any]]],\n",
    ") -> tuple[torch.Tensor, list[dict[str, Any]]]:\n",
    "    \"\"\"For use with Dataloader - keep targets as a list\"\"\"\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    targets = [item[1] for item in batch]\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "coco_loader = DataLoader(coco_dataset, batch_size=8,\n",
    "    collate_fn=collate_fn)\n",
    "next(iter(coco_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d333f",
   "metadata": {},
   "source": [
    "# MCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a797955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "mcd_root = data_path / \"coco/minecraft\"\n",
    "mcd = bb.MCDataset(mcd_root)\n",
    "img, target = mcd.coco_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2beeef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcd_loader = DataLoader(mcd, batch_size=8, collate_fn=bb.MCDataset.collate_fn)\n",
    "images, targets = next(iter(mcd_loader))\n",
    "# bb.plot_bb_grid(images, targets, mcd.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba1d8f",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18122d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "batch = [trainer.weights.transforms()(img)]\n",
    "# summary(trainer.model, input_data=[batch])\n",
    "summary(\n",
    "    trainer.model,\n",
    "    input_size=[1, 3, 640, 640],\n",
    "    col_names=[\n",
    "        \"input_size\",\n",
    "        \"output_size\",\n",
    "        # \"num_params\",\n",
    "        # \"params_percent\",\n",
    "        \"kernel_size\",\n",
    "        # \"mult_adds\",\n",
    "        \"trainable\",\n",
    "    ],\n",
    "    row_settings=[\n",
    "        # \"ascii_only\",\n",
    "        \"depth\",\n",
    "        \"var_names\",\n",
    "    ],\n",
    "    depth=10,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedfac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fcos\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, device=\"mps\") -> None:\n",
    "        self.device = torch.device(device)\n",
    "        # 11 classes - 10 plus background\n",
    "        num_classes = 11\n",
    "\n",
    "        self.weights = fcos.FCOS_ResNet50_FPN_Weights.COCO_V1\n",
    "        self.model = fcos.fcos_resnet50_fpn(weights=self.weights)\n",
    "        self.preprocess = self.weights.transforms()\n",
    "\n",
    "        # Conv2d(256, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.orig_cls_logits = self.model.head.classification_head.cls_logits\n",
    "        self.model.head.classification_head.cls_logits = torch.nn.Conv2d(\n",
    "            256, num_classes, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.model.head.classification_head.num_classes = num_classes\n",
    "\n",
    "        # Move to device after head is replaced\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        for param in self.model.head.classification_head.cls_logits.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        self.optimizer = torch.optim.AdamW(params=self.model.parameters(), lr=1e-4)\n",
    "\n",
    "    def infer(self, img: tv.tv_tensors.Image) -> Image.Image:\n",
    "        self.model.eval()\n",
    "        img = img.to(self.device)\n",
    "        batch = [self.preprocess(img)]\n",
    "        with torch.inference_mode():\n",
    "            prediction = self.model(batch)[0]\n",
    "        labels = [self.weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "        box = tv.utils.draw_bounding_boxes(\n",
    "            img,\n",
    "            boxes=prediction[\"boxes\"],\n",
    "            labels=labels,\n",
    "            colors=\"red\",\n",
    "            width=4,\n",
    "            font=\"/System/Library/Fonts/Helvetica.ttc\",  # macOS\n",
    "            font_size=20,\n",
    "        )\n",
    "        return v2.functional.to_pil_image(box.detach())\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "\n",
    "        for images, targets in tqdm(train_loader):\n",
    "            images = images.to(self.device)\n",
    "            targets = [\n",
    "                {\n",
    "                    # Handle images with no boxes\n",
    "                    \"boxes\": t.get(\"boxes\", torch.zeros(0, 4)).to(self.device),\n",
    "                    \"labels\": t.get(\"labels\", torch.zeros(0, dtype=torch.int64)).to(\n",
    "                        self.device\n",
    "                    ),\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "\n",
    "            # Forward pass of image through network and get output\n",
    "            batch = self.preprocess(images)\n",
    "            # torchvision models return loss in train mode.\n",
    "            loss_dict = self.model(batch, targets)\n",
    "            loss = sum(loss_dict.values())\n",
    "\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Backpropagate gradients\n",
    "            loss.backward()\n",
    "            # Do a single optimization step\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "img = mcd[0][0]\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52977cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.infer(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_one_epoch(mcd_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(mcd_loader))\n",
    "trainer.preprocess(images).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
