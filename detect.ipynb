{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02af6360",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eee628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Sequence\n",
    "\n",
    "from IPython.display import display # type: ignore\n",
    "import ipywidgets as widgets # type: ignore\n",
    "\n",
    "import torch\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "\n",
    "import tt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a9ad7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7020a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.home() / \"src/data\"\n",
    "mobs_path = data_path / \"mobs1/640\"\n",
    "\n",
    "IMAGE_FILES = sorted(list(mobs_path.iterdir()))\n",
    "\n",
    "CLASSES = [\n",
    "    'chicken',\n",
    "    'cow',\n",
    "    'creeper',\n",
    "    'enderman',\n",
    "    'pig',\n",
    "    'player',\n",
    "    'sheep',\n",
    "    'skeleton',\n",
    "    'spider',\n",
    "    'villager',\n",
    "    'zombie'\n",
    "    ]\n",
    "CLASSES.sort()\n",
    "CLASSES_MINECRAFT = [f\"minecraft {x}\" for x in CLASSES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = tt.ImageDirViewer(mobs_path)\n",
    "viewer.show_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cbaa4c",
   "metadata": {},
   "source": [
    "# Owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17510626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "\n",
    "class Owl:\n",
    "    # model_id = \"google/owlv2-base-patch16-ensemble\"\n",
    "    model_id = \"google/owlv2-large-patch14-ensemble\"\n",
    "    \n",
    "    def __init__(self, classes: list[str], threshold: float = 0.1):\n",
    "        self.processor = Owlv2Processor.from_pretrained(self.model_id)\n",
    "        self.model = Owlv2ForObjectDetection.from_pretrained(self.model_id).to(device)\n",
    "        self.classes = classes\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def detect(self, image: Image.Image) -> dict[Any, Any]:\n",
    "        texts = [self.classes]\n",
    "        inputs = self.processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "        target_sizes = torch.Tensor([image.size[::-1]])\n",
    "        # Convert outputs (bounding boxes and class logits) to Pascal VOC Format (xmin, ymin, xmax, ymax)\n",
    "        results = self.processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=self.threshold)\n",
    "        i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "        text = texts[i]\n",
    "        boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "        return results[i]\n",
    "\n",
    "    def infer(self, image_file: str | Path) -> Image.Image:\n",
    "        image_file = Path(image_file)\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        result = self.detect(image)\n",
    "\n",
    "        detections = sv.Detections.from_transformers(result)\n",
    "        # Create annotators\n",
    "        box_annotator = sv.BoxAnnotator()\n",
    "        label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "        # display(result)\n",
    "        # print(\"--------------------\")\n",
    "        # display(detections)\n",
    "\n",
    "        # Create labels\n",
    "        names = [self.classes[x] for x in detections.class_id]\n",
    "        labels = [f\"{name}: {conf:.2f}\" for name, conf in zip(names, detections.confidence)]\n",
    "\n",
    "        # # Annotate\n",
    "        annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "        annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "        return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08026f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce classes\n",
    "owl_remove = [\"creeper\", \"enderman\", \"player\", \"skeleton\", \"villager\", \"zombie\"]\n",
    "owl_classes = [x for x in CLASSES if x not in owl_remove]\n",
    "\n",
    "owl_classes_minecraft = [f\"minecraft {x}\" for x in owl_classes]\n",
    "\n",
    "owl1 = Owl(owl_classes_minecraft, threshold=0.4)\n",
    "fname = mobs_path / '06092b21-2024-10-20_22.22.09.png'\n",
    "image = owl1.infer(fname)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = tt.InferViewer(owl1.infer, mobs_path)\n",
    "viewer.show_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a5a8a",
   "metadata": {},
   "source": [
    "# Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOWorld\n",
    "\n",
    "fname = mobs_path / '06092b21-2024-10-20_22.22.09.png'\n",
    "# Initialize model\n",
    "model = YOLOWorld(\"yolov8x-worldv2.pt\")  # or yolov8l-worldv2.pt\n",
    "\n",
    "# Set classes\n",
    "model.set_classes([\"spider\", \"pig\", \"minecraft chicken\", \"cow\", \"creeper\", \"zombie\", \"skeleton\"])\n",
    "\n",
    "# Predict\n",
    "image = Image.open(fname).convert(\"RGB\")\n",
    "results = model.predict(image, conf=.04)\n",
    "annotated_bgr = results[0].plot()\n",
    "annotated_rgb = annotated_bgr[..., ::-1]\n",
    "display(Image.fromarray(annotated_rgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d8767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo:\n",
    "    model_id = \"yolov8x-worldv2.pt\"\n",
    "    # model_id = \"yolov8l-worldv2.pt\"\n",
    "    \n",
    "    def __init__(self, classes: list[str], conf=0.25):\n",
    "        self.model = YOLOWorld(self.model_id)\n",
    "        self.model.set_classes(classes)\n",
    "        self.conf = conf\n",
    "\n",
    "    def detect(self, image: Image.Image) -> dict[Any, Any]:\n",
    "        result = self.model.predict(image, conf=self.conf)\n",
    "        return result\n",
    "\n",
    "    def infer(self, image_file: str | Path) -> Image.Image:\n",
    "        image_file = Path(image_file)\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        results = self.detect(image)\n",
    "\n",
    "        # # Create annotators\n",
    "        annotated_bgr = results[0].plot()\n",
    "        annotated_rgb = annotated_bgr[..., ::-1]\n",
    "        return Image.fromarray(annotated_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo1 = Yolo(CLASSES_MINECRAFT, conf=.03)\n",
    "viewer = tt.InferViewer(yolo1.infer, mobs_path)\n",
    "viewer.show_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce68e4f",
   "metadata": {},
   "source": [
    "# Dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158281d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, GroundingDinoProcessor, AutoModelForZeroShotObjectDetection # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dino:\n",
    "    model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "    # model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "    \n",
    "    def __init__(self, classes: Sequence[str], threshold: float, text_threshold: float):\n",
    "        self.processor: GroundingDinoProcessor = AutoProcessor.from_pretrained(self.model_id)\n",
    "        self.model = AutoModelForZeroShotObjectDetection.from_pretrained(self.model_id).to(device)\n",
    "\n",
    "        self.classes = classes\n",
    "        self.threshold = threshold\n",
    "        self.text_threshold = text_threshold\n",
    "\n",
    "    def detect(self, image: Image.Image) -> dict[Any, Any]:\n",
    "        inputs = self.processor(images=image, text=self.classes, return_tensors=\"pt\").to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        results = self.processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            threshold=self.threshold,\n",
    "            text_threshold=self.text_threshold,\n",
    "            target_sizes=[image.size[::-1]]\n",
    "        )\n",
    "\n",
    "        result: dict[Any, Any] = results[0]\n",
    "        display(result)\n",
    "        for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n",
    "            box = [round(x, 2) for x in box.tolist()]\n",
    "            print(f\"Detected {labels} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def fix_grounding_dino_result(self, result: dict) -> tuple[dict, dict]:\n",
    "        \"\"\"Work around bug in grounding dino transformer. result[\"labels\"] are supposed to be ints.\"\"\"\n",
    "        # Create label mapping\n",
    "        unique_labels = list(set(result[\"text_labels\"]))\n",
    "        label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        id2label = {idx: label for label, idx in label2id.items()}\n",
    "        \n",
    "        # Fix result\n",
    "        result_fixed = result.copy()\n",
    "        result_fixed[\"labels\"] = torch.tensor(\n",
    "            [label2id[label] for label in result[\"text_labels\"]], \n",
    "            device=result[\"boxes\"].device\n",
    "        )\n",
    "        \n",
    "        return result_fixed, id2label\n",
    "\n",
    "    def infer(self, image_file: str | Path) -> Image.Image:\n",
    "        image_file = Path(image_file)\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        result = self.detect(image)\n",
    "\n",
    "        result_fixed, id2label = self.fix_grounding_dino_result(result)\n",
    "        detections = sv.Detections.from_transformers(result_fixed, id2label=id2label)\n",
    "        # Create annotators\n",
    "        box_annotator = sv.BoxAnnotator()\n",
    "        label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "        # Create labels\n",
    "        labels = [\n",
    "            f\"{label}: {score:.2f}\"\n",
    "            for label, score in zip(result[\"text_labels\"], result[\"scores\"])\n",
    "        ]\n",
    "\n",
    "        # Annotate\n",
    "        annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "        annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
    "        return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino1 = Dino(CLASSES, 0.3, 0.5)\n",
    "fname = mobs_path / '06092b21-2024-10-20_22.22.09.png'\n",
    "image = dino1.infer(fname)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = tt.InferViewer(dino1.infer, mobs_path)\n",
    "viewer.show_widget()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
